# --- Main Application Configuration ---
replicaCount: 1

image:
  repository: # To be filled in by the pipeline
  pullPolicy: IfNotPresent
  tag: "" # To be filled in by the pipeline

service:
  type: ClusterIP
  port: 80

# --- Advanced Scheduling for the Application ---
# (Usually, the stateless app doesn't need dedicated nodes, but the option is here)
app:
  nodeSelector: {}
  tolerations: []
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - hello-birthday
          topologyKey: "topology.kubernetes.io/zone"


# --- Database Configuration (Postgres with Zalando Operator) ---
postgres:
  enabled: true
  replicas: 3
  volumeSize: "10Gi"
  version: "15"
  s3WalBucket: "" # To be filled in by the pipeline
  teamId: "birthday-team"

  # --- NEW: Advanced Scheduling for the Database ---
  # This configuration ensures the DB pods run on dedicated nodes.
  scheduling:
    # Forces pods to run on nodes with this label.
    nodeSelector:
      workload: database

    # Allows pods to run on nodes with "taints".
    tolerations:
    - key: "workload"
      operator: "Equal"
      value: "database"
      effect: "NoSchedule"

    # Maintains high-availability dispersion within the dedicated nodes.
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: application
                operator: In
                values:
                - spilo
            topologyKey: "topology.kubernetes.io/zone"

  # Resource Requests and Limits (CRITICAL FOR PRODUCTION!)
  resources:
    requests:
      cpu: "500m"
      memory: "1Gi"
    limits:
      cpu: "1"
      memory: "2Gi"

  # Pod Disruption Budget for High Availability
  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1

  # Connection Pooler
  connectionPooler:
    enabled: true
    numberOfInstances: 2
    mode: "transaction"
    user: "pooler"
    resources:
      requests:
        cpu: "100m"
        memory: "100Mi"
      limits:
        cpu: "200m"
        memory: "200Mi"

  # Enable the metrics sidecar for Prometheus
  monitoring:
    enabled: true


# --- OpenTelemetry (ADOT) Collector Configuration ---
# Configuration for the ADOT sub-chart (inheritance mechanism)
adot:
  enabled: true # sub-chart is enabled
  roleArn: "" # fill in from pipeline
  prometheusEndpoint: "" # fill in from pipeline
  s3LogsBucket: "" # fill in from pipeline

  # Collector configuration
  collector:
    # The collector will run as a Deployment in the cluster.
    daemonSet: false
    deployment: true

    # Service Account configuration for the collector
    serviceAccount:
      create: true
      name: "adot-collector-sa" # Must match the name in the IAM role in Terraform
      annotations:
        # This annotation binds the service account to the IAM role
        eks.amazonaws.com/role-arn: "" # fill in from pipeline

    # Collector configuration itself
    # Here the telemetry pipelines are defined: receivers, processors, exporters.
    config:
      # Exporters: where to send the data?
      exporters:
        awsprometheusremotewrite:
          # The URL of the prometheus remote write endpoint.
          # Will be filled in from the pipeline.
          endpoint: ""
        awss3:
          aws_sdk:
            region: "eu-west-1"
            s3_force_path_style: true
          s3_uploader:
            s3_bucket: "" # To be filled in by the pipeline
            s3_prefix: "logs"
            s3_partition: "minute"

      # Receivers: What data do we want to collect?
      receivers:
        prometheus:
          config:
            scrape_configs:
              - job_name: 'hello-birthday-app'
                scrape_interval: 15s
                # The collector will look for pods with the annotation `app.kubernetes.io/name=hello-birthday`
                # and will scrape metrics from them.
                kubernetes_sd_configs:
                  - role: pod
                relabel_configs:
                  - source_labels: [__meta_kubernetes_pod_annotation_app_kubernetes_io_name]
                    action: keep
                    regex: hello-birthday
        filelog:
          include: [ /var/log/pods/*/*/*.log ] # standard path for container logs in Kubernetes
          start_at: beginning

      # Services: enables the pipelines for the collector
      service:
        pipelines:
          metrics:
            receivers: [ prometheus ]
            exporters: [ awsprometheusremotewrite ]

          logs:
            receivers: [ filelog ]
            exporters: [ awss3 ]

# --- Ingress Configuration ---
ingress:
  enabled: true
  className: "alb" # Specifies that we want to use the AWS Load Balancer Controller
  annotations:
    # Specific annotations for the AWS Load Balancer Controller
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
  hosts:
    - host: hello-birthday.yourdomain.com # <-- CHANGE THIS!
      paths:
        - path: /
          pathType: Prefix
  tls: []